# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eREwEiLzdv2GBbaU1QIVDgSS7vL6_9rx
"""

import os
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
from flask import Flask, request, jsonify
from sklearn.metrics.pairwise import cosine_similarity

# Initialize Flask app
app = Flask(__name__)

# Initialize BLIP model for image captioning (from HuggingFace)
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Load Universal Sentence Encoder for text embeddings
embed_model = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")

# Function to preprocess images for BLIP
def preprocess_image(image_path):
    img = Image.open(image_path).convert("RGB")
    return img

# Function to generate captions using BLIP model
def generate_caption(image_path):
    raw_image = preprocess_image(image_path)
    inputs = processor(raw_image, return_tensors="pt")
    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption

# Function to compute similarity between two texts using Universal Sentence Encoder
def compute_similarity(text1, text2):
    embeddings = embed_model([text1, text2])
    sim_score = cosine_similarity([embeddings[0].numpy()], [embeddings[1].numpy()])[0][0]
    return sim_score

@app.route('/upload', methods=['POST'])
def upload_images():
    # Check if the folder is in the request
    if 'images' not in request.files:
        return jsonify({"error": "No images part"}), 400

    # Retrieve the images and search query from the request
    images = request.files.getlist('images')
    search_query = request.form.get('search_query')

    # List to store image paths, captions, and features
    captions = []
    image_paths = []

    # Temporary directory to save images
    temp_folder = 'temp_images'
    os.makedirs(temp_folder, exist_ok=True)

    # Process each image in the folder
    for img in images:
        img_path = os.path.join(temp_folder, img.filename)
        img.save(img_path)

        # Generate caption for the image using BLIP
        caption = generate_caption(img_path)
        captions.append(caption)
        image_paths.append(img_path)

    # Now compare each caption with the search query
    similarities = []
    for caption in captions:
        sim_score = compute_similarity(search_query, caption)
        similarities.append(sim_score)

    # Find the image with the highest similarity
    best_match_index = np.argmax(similarities)
    best_match_image = image_paths[best_match_index]
    best_caption = captions[best_match_index]
    best_similarity = similarities[best_match_index]

    return jsonify({
        "best_match_image": best_match_image,
        "caption": best_caption,
        "similarity_score": best_similarity
    })

if __name__ == '__main__':
    app.run(debug=True)